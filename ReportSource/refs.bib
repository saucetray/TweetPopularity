@inproceedings{bertweet,
title     = {{BERTweet: A pre-trained language model for English Tweets}},
author    = {Dat Quoc Nguyen and Thanh Vu and Anh Tuan Nguyen},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
year      = {2020},
pages     = {9--14}
}
@inproceedings{levi,
author = {Levi, Or and Hosseini, Pedram and Diab, Mona and Broniatowski, David},
year = {2019},
month = {01},
pages = {31-35},
title = {Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues},
doi = {10.18653/v1/D19-5004}
}

@article{CLARK20161,
title = "Sifting robotic from organic text: A natural language approach for detecting automation on Twitter",
journal = "Journal of Computational Science",
volume = "16",
pages = "1 - 7",
year = "2016",
issn = "1877-7503",
doi = "https://doi.org/10.1016/j.jocs.2015.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S1877750315300363",
author = "Eric M. Clark and Jake Ryland Williams and Chris A. Jones and Richard A. Galbraith and Christopher M. Danforth and Peter Sheridan Dodds",
abstract = "Twitter, a popular social media outlet, has evolved into a vast source of linguistic data, rich with opinion, sentiment, and discussion. Due to the increasing popularity of Twitter, its perceived potential for exerting social influence has led to the rise of a diverse community of automatons, commonly referred to as bots. These inorganic and semi-organic Twitter entities can range from the benevolent (e.g., weather-update bots, help-wanted-alert bots) to the malevolent (e.g., spamming messages, advertisements, or radical opinions). Existing detection algorithms typically leverage metadata (time between tweets, number of followers, etc.) to identify robotic accounts. Here, we present a powerful classification scheme that exclusively uses the natural language text from organic users to provide a criterion for identifying accounts posting automated messages. Since the classifier operates on text alone, it is flexible and may be applied to any textual data beyond the Twittersphere."
}

@article{turc2019,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962v2 },
  year={2019}
}

@article{DBLP,
  author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{victoria,
author = {Conroy, Nadia and Rubin, Victoria and Chen, Yimin},
year = {2015},
month = {10},
pages = {},
title = {Automatic Deception Detection: Methods for Finding Fake News}
}
@article{word2vec,
  author    = {Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  journal   = {CoRR},
  volume    = {abs/1310.4546},
  year      = {2013},
  url       = {http://arxiv.org/abs/1310.4546},
  archivePrefix = {arXiv},
  eprint    = {1310.4546},
  timestamp = {Mon, 13 Aug 2018 16:47:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MikolovSCCD13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}